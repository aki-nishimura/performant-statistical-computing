---
title: "Hardware aware computing"
subtitle: "Know thy computer and design _actually_&#8239; fast algorithms"
author: "Aki Nishimura"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "extra.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      slideNumberFormat: "%current%"
      countIncrementalSlides: false
    includes:
      before_body: mathjax_config.html
---

```{r setup, include = FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c('Rcpp', 'RcppEigen', 'RcppArmadillo', 'microbenchmark')
install_and_load_packages(required_packages)
```

```{r, include=FALSE}
# Cache Rcpp compilations
knitr::opts_chunk$set(cache = TRUE)

# Temporarily supress warnings because compilations generate a lot of them
default_warn_opt <- getOption("warn") 
options(warn = -1) 

# Print outputs without "##"
knitr::opts_chunk$set(comment = '')
```

# Word of wisdom on optimization

<p style="margin-top:12ex; font-size:26pt; font-family:garamond; font-style:italic;", class="center">
"Premature optimization is the root of all evil."
</p>

<p style="margin-top:1ex; font-size:25pt; font-family:'Times New Roman';", class="right"> &mdash; Donald Knuth </p>

---

# Word of wisdom on optimization

<blockquote>
<p style="margin-top:10ex; font-size:24pt; font-family:garamond; font-style:italic;">
We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.
</p>
</blockquote>

<p style="margin-top:1ex; font-size:25pt; font-family:'Times New Roman';", class="right"> &mdash; Donald Knuth </p>

---

# Word of wisdom on optimization

<blockquote>
<p style="font-size:24pt; font-family:garamond; font-style:italic;">
Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. 
</p>
<p style="margin-top:-.5ex; font-size:24pt; font-family:garamond; font-style:italic;">
We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.
</p>
</blockquote>

<p style="margin-top:-.5ex; font-size:25pt; font-family:'Times New Roman';", class="right"> &mdash; Donald Knuth </p>

---

# Word of wisdom on code readability

<p style="margin-top:11.5ex; font-size:24pt; font-family:garamond; font-style:italic;", class="center">
"Programs are meant to be read by humans and<br> only incidentally for computers to execute."
</p>
<p style="margin-top:1ex; font-size:25pt; font-family:'Times New Roman';", class="right"> &mdash; Donald Knuth </p>

---
class: center, middle, inverse

# Same functionality, varying performance

## (I mean, duh. But&hellip; umm, why?)

---

# "Vectorize!" but what does it mean, really?

e.g. `axpy` from _Basic Linear Algebra Subprograms_ (BLAS):
$$\bm{y} \gets a \bm{x} + \bm{y}$$

```{r}
axpy <- function(a, x, y) {
  for (i in 1:length(x)) {
    y[i] <- a * x[i] + y[i] 
  }
  return(y)
}

axpy_vec <- function(a, x, y) {
  y <- a * x + y
  return(y)
}
```

---

# "Vectorize!" but what does it mean, really?

```{r}
n <- 10^6
a <- 3.14
x <- rep(1, n)
y <- rep(0.00159, n)
```

--

```{r, eval=FALSE}
bench::mark(axpy(a, x, y))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(axpy(a, x, y))
)
```

```{r, eval=FALSE}
bench::mark(axpy_vec(a, x, y))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(axpy_vec(a, x, y))
)
```

---

# "Compiled code is faster" but why?

```{r}
sourceCpp(file.path('src', 'axpy_c.cpp'))
```
<p style="margin-top: -.5ex;"> </p>

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'axpy_c.cpp')), sep = '\n')
```

--

```{r, eval=FALSE}
bench::mark(axpy_c(a, x, y))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(axpy_c(a, x, y))
)
```

---

# "Compiled code is faster" but why?

```{r, eval=FALSE}
sourceCpp(file.path('src', 'axpy_c.cpp'))
```
<p style="margin-top: -.5ex;"> </p>

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'axpy_c.cpp')), sep = '\n')
```

<p style="margin-top: -.5ex;"> </p>

**Note:** This function directly modifies `y`.
<p style="margin-top: -1ex;"> </p>
```{r}
y <- rep(0.00159, n)
invisible(axpy_c(a, x, y))
head(y)
```

---

# Is a dedicated linear algebra library faster?

```{r}
sourceCpp(file.path('src', 'axpy_eigen.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'axpy_eigen.cpp')), sep = '\n')
```

--

```{r, eval=FALSE}
bench::mark(axpy_eigen(a, x, y))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(axpy_eigen(a, x, y))
)
```

---

# Is a dedicated linear algebra library faster?

```{r}
sourceCpp(file.path('src', 'axpy_arma.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'axpy_arma.cpp')), sep = '\n')
```

```{r, eval=FALSE}
bench::mark(axpy_arma(a, x, y))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(axpy_arma(a, x, y))
)
```

```{r, include=FALSE}
# No more Rcpp compilations; stop caching
knitr::opts_chunk$set(cache = FALSE)

# Turn back on warnings
options(warn = default_warn_opt)
```

---
layout: false
class: center, middle, inverse

# A bit of assembly code

---

# Quick peek into CPU
<img src="figure/computing_at_low_level/cpu_diagram.jpeg" alt="" class="center" style="width: 85%;"/>

---

# What's machine doing behind the scene?

To foster mutual understanding between us and machine, let's look at simple _assembly code_.

--

<img src="figure/computing_at_low_level/divide_int_by_two.png" class="center" style="width: 100%;"/>

---

# Registers: most accessible data storage
<img src="figure/computing_at_low_level/x86_32bits_registers.png" class="center" style="width: 85%;"/>

---

<!-- Let's now try to decifer what our machine friend is doing: -->
# Division via bit-shift
.pull-left[
  <img src="figure/computing_at_low_level/divide_int_by_two_assembly_only.png" class="left" style="width: 100%;"/> 
]
--
.pull-right[
  "shr" = _logical shift_ to right
  
  <img src="figure/computing_at_low_level/right_logical_shift.png" class="center" style="width: 100%;"/>
]

---

# Division via bit-shift
.pull-left[
  <img src="figure/computing_at_low_level/divide_int_by_two_assembly_only.png" class="left" style="width: 100%;"/> 
]
.pull-right[
  <img src="figure/computing_at_low_level/twos_complement_representation_of_integer.png" class="center" style="width: 100%;"/>
]

---

# Division via bit-shift
.pull-left[
  <img src="figure/computing_at_low_level/divide_int_by_two_assembly_only.png" class="left" style="width: 100%;"/> 
]
.pull-right[
  "sar" = _arith. shift_ to right
  
  <img src="figure/computing_at_low_level/right_arithmetic_shift.png" class="center" style="width: 100%;"/>
]
<!-- https://stackoverflow.com/questions/40638335/why-does-the-compiler-generate-a-right-shift-by-31-bits-when-dividing-by-2 -->

---

# More general divisions

In order to optimize division as bit-shift, a compiler needs to know we are dividing by 2 and not by a generic integer:

<img src="figure/computing_at_low_level/divide_int_by_int.png" class="center" style="width: 100%;"/> 

---

# More general divisions

Floating point arithmetic uses completely different logic:

<img src="figure/computing_at_low_level/divide_double_by_two.png" class="center" style="width: 75%;"/> 

---
layout: false
class: center, middle, inverse

# Pipelining "fetch-decode-execute-store"
## How to keep your CPU "executing"

---
layout: true

# Example: effect of details in "tight loops"

---

Consider two implementations of a `sign` function:
```{r, message=FALSE, cache=TRUE}
sourceCpp(file.path('src', 'sign.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'sign.cpp'))[5:20], sep = '\n')
```

---

Consider two implementations of a `sign` function:
```{r, eval=FALSE}
sourceCpp(file.path('src', 'sign.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'sign.cpp'))[22:30], sep = '\n')
```

---

Let's compare their performances:
```{r, echo=FALSE}
x <- rnorm(10^6)
```

```{r, eval=FALSE}
x <- rnorm(10^6)
bench::mark(sign_via_if(x))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(sign_via_if(x))
)
```

```{r, eval=FALSE}
bench::mark(sign_via_diff(x))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(sign_via_diff(x))
)
```

---

Another run just to make sure... oh, what happened?
```{r, echo=FALSE}
x <- rep(1, 10^6)
```

```{r, eval=FALSE}
x <- rep(1, 10^6)
bench::mark(sign_via_if(x))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(sign_via_if(x))
)
```

```{r, eval=FALSE}
bench::mark(sign_via_diff(x))
```

```{r, echo=FALSE}
summarize_benchmark(
  bench::mark(sign_via_diff(x))
)
```

---
layout: false

# Instruction cycle:<br> $\quad$ fetch-decode-execute-store

<img src="figure/computing_at_low_level/fetch_decode_execute_store_cycle.png" class="center" style="width: 70%;"/> 

---

# Instruction cycle:<br> $\quad$ fetch-decode-execute-store

<img src="figure/computing_at_low_level/fetch_decode_execute_store_cycle_more_details.jpeg" class="center" style="width: 100%;"/> 

---

# "Instruction pipelining" to avoid idle time

<p style="margin-top: -1ex;"> </p>
<img src="figure/computing_at_low_level/instruction_pipelining.png" class="center" style="width: 70%;"/> 

---
layout: true

# Keep it simple & predictable for pipelining

---

You can't pipeline if the next action depends on curr one!

---

Let's take another look at the two `sign` implementations:

<img src="figure/computing_at_low_level/sign_via_diff.png" class="center" style="width: 100%;"/> 

---

Let's take another look at the two `sign` implementations:

<img src="figure/computing_at_low_level/sign_via_if.png" class="center" style="width: 100%;"/> 

---

Let's take another look at the two `sign` implementations:

<img src="figure/computing_at_low_level/sign_via_if_optimized.png" class="center" style="width: 100%;"/> 


---
class: center, middle, inverse

# It's about time: <br> worry less about number of arithmetic ops & more about data motion efficiency

---

# CPU got communication issues with RAM <br> .small[(a.k.a. von Neumann bottleneck)]
<!-- <p style="margin-top: -1ex;"> </p> -->
<img src="figure/computing_at_low_level/cpu_diagram_with_memory.jpeg" alt="" class="center" style="width: 90%;"/>

---
layout: true

# Example: row- vs. column-oriented matvec

---

<!-- <p style="margin-top: 3ex;"> </p> -->
Matvec $\bm{x} \to \bm{A} \bm{x}$ viewed as multiplying rows of $\bm{A}$ with $\bm{x}$:

<img src="figure/computing_at_low_level/row_oriented_matvec.png" class="center" style="width: 85%;"/> 

---

Viewed as adding up cols of $\bm{A}$ multiplied by entries of $\bm{x}$:

<img src="figure/computing_at_low_level/col_oriented_matvec.png" class="center" style="width: 100%;"/> 

<p style="margin-top:-.5ex;"> </p>

.pull-left[
```{r, eval=FALSE}
# Row-oriented matvec
Ax <- rep(0, n_row)
for (i in 1:n_row) {
  for (j in 1:n_col) {
    Ax[i] <- Ax[i] + 
      A[i, j] * x[j]
  }
}
```
]

.pull-right[
```{r, eval=FALSE}
# Column-oriented matvec
Ax <- rep(0, n_row)
for (j in 1:n_col) {
  for (i in 1:n_row) {
    Ax[i] <- Ax[i] + 
      A[i, j] * x[j]
  }
}
```
]

---

Here is how a row-oriented matvec looks like:
```{r}
row_oriented_matvec <- function(A, v) {
  Av <- rep(0., n_row)
  for (i in 1:n_row) {
    Av[i] <- sum(A[i, ] * v)
  }
  return(Av)
}
```
and a column-oriented matvec:
```{r}
col_oriented_matvec <- function(A, v) {
  Av <- rep(0., n_row)
  for (j in 1:n_col) {
    Av <- Av + A[, j] * v[j]
  }
  return(Av)
}
```

---

Let's compare their performence:

```{r}
n_row <- 4096L
n_col <- 4096L
A <- matrix(rnorm(n_row * n_col), n_row, n_col)
v <- rnorm(n_row)
```

--

```{r, eval=FALSE}
bench::mark(
  row_oriented_matvec(A, v)
)
```

```{r, echo=FALSE}
bench_output <- bench::mark(
  Av_via_row <- row_oriented_matvec(A, v)
)
summarize_benchmark(bench_output)
```

```{r, eval=FALSE}
bench::mark(
  col_oriented_matvec(A, v)
)
```

```{r, echo=FALSE}
bench_output <- bench::mark(
  Av_via_col <- col_oriented_matvec(A, v)
)
summarize_benchmark(bench_output)
```

```{r, echo=FALSE}
stopifnot(are_all_close(Av_via_col, Av_via_row))
```

---

Huh, maybe because it's not compiled code? Let's check.

```{r, message=FALSE}
sourceCpp(file.path('src', 'matvec.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'matvec.cpp'))[6:17], sep = '\n')
```

---

Huh, maybe because it's not compiled code? Let's check.

```{r, message=FALSE}
sourceCpp(file.path('src', 'matvec.cpp'))
```

```{r, comment='', echo=FALSE}
cat(readLines(file.path('src', 'matvec.cpp'))[20:31], sep = '\n')
```

---

Huh, maybe because it's not compiled code? Let's check.

```{r, eval=FALSE}
bench::mark(
  row_oriented_matvec_c(A, v)
)
```

```{r, echo=FALSE}
bench_output <- bench::mark(
  Av_via_row <- row_oriented_matvec_c(A, v)
)
summarize_benchmark(bench_output)
```

```{r, eval=FALSE}
bench::mark(
  col_oriented_matvec_c(A, v)
)
```

```{r, echo=FALSE}
bench_output <- bench::mark(
  Av_via_col <- col_oriented_matvec_c(A, v)
)
summarize_benchmark(bench_output)
```

--

```{r, eval=FALSE}
bench::mark(A %*% v)
```

```{r, echo=FALSE}
summarize_benchmark(bench::mark(A %*% v))
```

---
layout: false

# Challenge: feeding data fast enough to CPU

<img src="figure/computing_at_low_level/cache_hierarchy_single_core.png" alt="" class="center" style="width: 70%;"/>

---

# Effect of cache miss and memory latency

Consider the following situation in your computer:
* CPU keeps itself busy if all data were in L1 cache,
but need 10 clock cycles to fetch data from L2 cache.
--

* On average, CPU finds 90% of necessary data in L1 cache, but have to look L2 cache for remaining 10%.

--

How long does it take for CPU to complete 10&nbsp;operations?
<!-- "how long" = "how many clock cycles" -->

---

# Lesson: optimize your code for data motion

Turns out R arrays are stored in _column major_ order:

<p style="margin-top: 3ex; margin-bottom: 3ex;"> 
<img src="figure/computing_at_low_level/column_major_matrix.png" alt="" class="center" style="width: 100%;"/>
</p>

--

So it is more efficient to access its elements column-wise!

---

# Lesson: optimize your code for data motion

In C and Python, arrays are stored in _row major_ order:

<p style="margin-top: 3ex; margin-bottom: 3ex;"> 
<img src="figure/computing_at_low_level/row_major_matrix.png" alt="" class="center" style="width: 100%;"/>
</p>

Make sure you choose the right order for double for-loops!
(Stan uses C arrays, for example.)

---

# Things get even more complicated when trying to parallelize over multiple CPUs

<p style="margin-top: 3ex;"> </p>

<img src="figure/computing_at_low_level/cache_hierarchy_multi_core.png" alt="" class="center" style="width: 100%;"/>

---

<p style="margin-top: 3ex;"> </p>
<img src="figure/computing_at_low_level/HPCG_results_Nov_2016.png" alt="" class="center" style="width: 100%;"/>

---

<p style="margin-top: 4ex;"> </p>

.large[**Nov 2020 Ranking:**]<br> 
$\quad$ .large[**Actual over theoretical performance**]

<img src="figure/computing_at_low_level/highest_hpcg_to_peak_ratio_among_top500_Nov_2020.jpeg" alt="" class="center" style="width: 100%;"/>

